# Swin-B fine-tuning configuration
seed: 42
data_dir: data
train_dir: data/train
val_dir: data/val
test_dir: data/test

img_size: 224
batch_size: 16
num_workers: 4

model_name: swin_base_patch4_window7_224.ms_in22k  # Swin-B pretrained on ImageNet-22k (timm)
pretrained: true
use_timm_augment: true   # leverage timm's Swin augmentation recipe

epochs: 60
learning_rate: 0.00035
weight_decay: 0.05
label_smoothing: 0.1
loss_type: cross_entropy   # mixup enabled below, so SoftTargetCrossEntropy will be used
focal_gamma: 2.0
focal_alpha: null
warmup_epochs: 8
warmup_epochs_phase1: 3
patience: 12
use_amp: true
max_grad_norm: 1.0
finetune_lr: 0.0001
mixup_alpha: 0.2
cutmix_alpha: 0.0
mixup_prob: 0.6
mixup_switch_prob: 0.5
mixup_mode: batch
use_weighted_sampler: true

device: cuda
save_dir: models
outputs_dir: outputs
resume_from: null       # optional path to a checkpoint to resume training

mlflow:
  enabled: true
  tracking_uri: http://localhost:5000
  experiment_name: medicinal-plants
  run_name: swin-b-finetune
  artifact_subdir: checkpoints
  eval_artifact_subdir: eval
  register_model: true
  registered_model_name: medicinal-plant-classifier
  model_artifact_subdir: model
