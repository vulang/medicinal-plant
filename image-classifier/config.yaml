# Training configuration
seed: 42
data_dir: data
train_dir: data/train
val_dir: data/val
test_dir: data/test

img_size: 224
batch_size: 32
num_workers: 2

model_name: swin_b   # choices: resnet18, resnet50, efficientnet_b0, mobilenet_v3_small, convnext_tiny/small/base/large, convnextv2_base.fcmae_ft_in22k_in1k, swin_t/swin_b, vit_base_patch16_224 (or other timm vit/deit/swin)
pretrained: true
use_timm_augment: true   # set true to use timm's RandomResizedCrop-style augmentation (more aggressive)

epochs: 40
learning_rate: 0.0005
weight_decay: 0.0001
label_smoothing: 0.1
loss_type: focal   # cross_entropy | focal (focal ignored when mixup/cutmix enabled)
focal_gamma: 2.0
focal_alpha: null         # optional scalar weight for focal loss; null to disable
warmup_epochs: 5
warmup_epochs_phase1: 3
patience: 10
use_amp: true
max_grad_norm: 1.0
finetune_lr: null        # set to a float (e.g., 0.0001) to drop LR before fine-tuning all layers
mixup_alpha: 0        # >0 enables mixup
cutmix_alpha: 0        # >0 enables cutmix
mixup_prob: 0.6          # probability of applying mixup/cutmix to a batch
mixup_switch_prob: 0.5   # probability of switching to the other augmentation at runtime
mixup_mode: batch        # batch | pair | elem
use_weighted_sampler: true  # set true to rebalance classes via WeightedRandomSampler

device: cuda           # auto | cpu | cuda | mps
save_dir: models
outputs_dir: outputs

mlflow:
  enabled: true
  tracking_uri: null      # e.g., http://127.0.0.1:5000 or leave null for local ./mlruns
  experiment_name: medicinal-plants
  run_name: null          # optionally override per run
  artifact_subdir: checkpoints
