# ViT-B/16 (ImageNet-22k) fine-tuning configuration
seed: 42
data_dir: data
train_dir: data/train
val_dir: data/val
test_dir: data/test

img_size: 224
batch_size: 16
num_workers: 4

model_name: vit_base_patch16_224_in21k  # ViT-B/16 pretrained on ImageNet-22k (timm)
pretrained: true
use_timm_augment: true   # use timm's ViT-friendly augmentation recipe

epochs: 80
learning_rate: 0.0002
weight_decay: 0.05
label_smoothing: 0.1
loss_type: cross_entropy   # mixup enabled below, so SoftTargetCrossEntropy will be used
focal_gamma: 2.0
focal_alpha: null
warmup_epochs: 10
warmup_epochs_phase1: 0
patience: 12
use_amp: true
max_grad_norm: 1.0
finetune_lr: 0.00005
mixup_alpha: 0.6
cutmix_alpha: 1.0
mixup_prob: 0.9
mixup_switch_prob: 0.5
mixup_mode: batch
use_weighted_sampler: true

device: cuda
save_dir: models
outputs_dir: outputs
resume_from: null       # optional path to a checkpoint to resume training

mlflow:
  enabled: true
  tracking_uri: http://localhost:5000
  experiment_name: medicinal-plants
  run_name: vit-b16-in21k-finetune
  artifact_subdir: checkpoints
  eval_artifact_subdir: eval
  register_model: true
  registered_model_name: medicinal-plant-classifier
  model_artifact_subdir: model
